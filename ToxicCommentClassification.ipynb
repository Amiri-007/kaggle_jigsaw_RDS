{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 8076,
          "databundleVersionId": 44219,
          "sourceType": "competition"
        },
        {
          "sourceId": 12500,
          "databundleVersionId": 1375107,
          "sourceType": "competition"
        },
        {
          "sourceId": 8593703,
          "sourceType": "datasetVersion",
          "datasetId": 5140771
        }
      ],
      "dockerImageVersionId": 30716,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "ToxicCommentClassification",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Amiri-007/kaggle_jigsaw_RDS/blob/master/ToxicCommentClassification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE\n",
        "# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.\n",
        "import kagglehub\n",
        "kagglehub.login()\n"
      ],
      "metadata": {
        "id": "39Jm1Fl6u2OG"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "jigsaw_unintended_bias_in_toxicity_classification_path = kagglehub.competition_download('jigsaw-unintended-bias-in-toxicity-classification')\n",
        "mahmoudelkarargy_dataaugumented_cleaned_path = kagglehub.dataset_download('mahmoudelkarargy/dataaugumented-cleaned')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "_rGmv4F-u2OI"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchmetrics\n",
        "!pip install loguru"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-03T11:46:44.863017Z",
          "iopub.execute_input": "2024-06-03T11:46:44.863388Z",
          "iopub.status.idle": "2024-06-03T11:47:09.085234Z",
          "shell.execute_reply.started": "2024-06-03T11:46:44.863358Z",
          "shell.execute_reply": "2024-06-03T11:47:09.084066Z"
        },
        "trusted": true,
        "id": "-wgLFsjtu2OI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import datetime\n",
        "from typing import Any, Union, Dict, List\n",
        "import uuid\n",
        "import json\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torchtext\n",
        "import nltk\n",
        "import sklearn\n",
        "import transformers\n",
        "import torchmetrics as tm\n",
        "from torchmetrics import MetricCollection, Metric, Accuracy, Precision, Recall, AUROC, HammingDistance, F1Score, ROC, PrecisionRecallCurve\n",
        "\n",
        "\n",
        "from loguru import logger\n",
        "from tqdm.auto import tqdm\n",
        "tqdm.pandas()\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-03T11:47:09.087305Z",
          "iopub.execute_input": "2024-06-03T11:47:09.087633Z",
          "iopub.status.idle": "2024-06-03T11:47:09.097051Z",
          "shell.execute_reply.started": "2024-06-03T11:47:09.087604Z",
          "shell.execute_reply": "2024-06-03T11:47:09.09621Z"
        },
        "trusted": true,
        "id": "NqFRdPk6u2OJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "import unicodedata\n",
        "import seaborn as sns\n",
        "import zipfile\n",
        "import os"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-03T11:47:09.098162Z",
          "iopub.execute_input": "2024-06-03T11:47:09.098491Z",
          "iopub.status.idle": "2024-06-03T11:47:09.107317Z",
          "shell.execute_reply.started": "2024-06-03T11:47:09.098467Z",
          "shell.execute_reply": "2024-06-03T11:47:09.10659Z"
        },
        "trusted": true,
        "id": "iGnqNcE7u2OJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_path = \"/kaggle/input/jigsaw-toxic-comment-classification-challenge\"\n",
        "\n",
        "# Specify the extraction directory\n",
        "extract_dir = \"/kaggle/working/unzipped_files\"\n",
        "\n",
        "# Create the extraction directory if it doesn't exist\n",
        "os.makedirs(extract_dir, exist_ok=True)\n",
        "\n",
        "# Iterate through the dataset directory to find and unzip all zip files\n",
        "for file_name in os.listdir(dataset_path):\n",
        "    if file_name.endswith(\".zip\"):\n",
        "        zip_file_path = os.path.join(dataset_path, file_name)\n",
        "        print(f\"Unzipping {zip_file_path}...\")\n",
        "        with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "            zip_ref.extractall(extract_dir)\n",
        "\n",
        "print(\"All files unzipped successfully!\")\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-03T07:06:27.109719Z",
          "iopub.execute_input": "2024-06-03T07:06:27.11035Z",
          "iopub.status.idle": "2024-06-03T07:06:28.183664Z",
          "shell.execute_reply.started": "2024-06-03T07:06:27.110317Z",
          "shell.execute_reply": "2024-06-03T07:06:28.182681Z"
        },
        "trusted": true,
        "id": "AbEL5VgQu2OJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset_path = \"/kaggle/input/jigsaw-unintended-bias-in-toxicity-classification\"\n",
        "dataset_path = \"/kaggle/working/unzipped_files\"\n",
        "\n",
        "CUSTOME_NAME = \"roberta-fl-augumented\"\n",
        "\n",
        "# Dataset\n",
        "DATA_DIR_PATH = dataset_path\n",
        "TRAIN_DATASET_PATH = os.path.join(DATA_DIR_PATH, \"train.csv\")\n",
        "# TRAIN_DATASET_PATH = \"/kaggle/working/augumented_by_25k_small_data.csv\"\n",
        "TEST_DATASET_PATH = os.path.join(DATA_DIR_PATH, \"test.csv\")\n",
        "\n",
        "#LABEL_LIST = ['severe_toxicity', 'obscene', 'threat', 'insult', 'identity_attack', 'sexual_explicit']\n",
        "LABEL_LIST = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-03T11:47:09.109368Z",
          "iopub.execute_input": "2024-06-03T11:47:09.109677Z",
          "iopub.status.idle": "2024-06-03T11:47:09.117975Z",
          "shell.execute_reply.started": "2024-06-03T11:47:09.109654Z",
          "shell.execute_reply": "2024-06-03T11:47:09.117105Z"
        },
        "trusted": true,
        "id": "aCv4xVm3u2OK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Session\n",
        "SESSION_DIR_PATH = os.path.abspath(\"./session\")\n",
        "SESSION_DATETIME = datetime.datetime.now().strftime(\"%Y-%m-%dT%H-%M-%S-%f\")\n",
        "SESSION_NAME = f\"{CUSTOME_NAME}_{SESSION_DATETIME}\"\n",
        "CURRENT_SESSION_DIR_PATH = os.path.join(SESSION_DIR_PATH, SESSION_NAME)\n",
        "# Cr√©er le dossier de la session\n",
        "os.makedirs(CURRENT_SESSION_DIR_PATH, exist_ok=True)\n",
        "\n",
        "# Architecture de fichier dans `CURRENT_SESSION_DIR_PATH`\n",
        "LOG_FILE_NAME = f\"{SESSION_NAME}.loguru.log\"\n",
        "MODEL_FILE_NAME = f\"{SESSION_NAME}.model\"\n",
        "TEST_FILE_NAME = f\"{SESSION_NAME}.test.csv\"\n",
        "VALIDATION_DATASET_NAME = f\"{SESSION_NAME}.jigsaw2019-validation.csv\"\n",
        "VALIDATION_FILE_NAME = f\"{SESSION_NAME}.validation.csv\"\n",
        "METRIC_FILE_NAME = f\"{SESSION_NAME}.metric.json\"\n",
        "LOG_FILE_PATH = os.path.join(CURRENT_SESSION_DIR_PATH, LOG_FILE_NAME)\n",
        "MODEL_FILE_PATH = os.path.join(CURRENT_SESSION_DIR_PATH, MODEL_FILE_NAME)\n",
        "TEST_FILE_PATH = os.path.join(CURRENT_SESSION_DIR_PATH, TEST_FILE_NAME)\n",
        "VALIDATION_DATASET_FILE_PATH = os.path.join(CURRENT_SESSION_DIR_PATH, VALIDATION_DATASET_NAME)\n",
        "VALIDATION_FILE_PATH = os.path.join(CURRENT_SESSION_DIR_PATH, VALIDATION_FILE_NAME)\n",
        "METRIC_FILE_PATH = os.path.join(CURRENT_SESSION_DIR_PATH, METRIC_FILE_NAME)\n",
        "\n",
        "# CUDA\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-03T11:47:09.119189Z",
          "iopub.execute_input": "2024-06-03T11:47:09.119549Z",
          "iopub.status.idle": "2024-06-03T11:47:09.130233Z",
          "shell.execute_reply.started": "2024-06-03T11:47:09.11952Z",
          "shell.execute_reply": "2024-06-03T11:47:09.12935Z"
        },
        "trusted": true,
        "id": "l4iXg5FVu2OK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logger.add(LOG_FILE_PATH, level=\"TRACE\")\n",
        "logger.info(f\"{SESSION_NAME=}\")\n",
        "logger.info(f\"{TRAIN_DATASET_PATH=}\")\n",
        "logger.info(f\"{TEST_DATASET_PATH=}\")\n",
        "logger.info(f\"{CURRENT_SESSION_DIR_PATH=}\")\n",
        "logger.info(f\"{LABEL_LIST=}\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-03T11:47:09.131218Z",
          "iopub.execute_input": "2024-06-03T11:47:09.131477Z",
          "iopub.status.idle": "2024-06-03T11:47:09.159436Z",
          "shell.execute_reply.started": "2024-06-03T11:47:09.131438Z",
          "shell.execute_reply": "2024-06-03T11:47:09.158597Z"
        },
        "trusted": true,
        "id": "KaeQIuhRu2OK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logger.info(f\"Checking consistency...\")\n",
        "\n",
        "\n",
        "if not os.path.exists(TRAIN_DATASET_PATH):\n",
        "    logger.critical(f\"Train dataset does not exist !\")\n",
        "    raise RuntimeError(\"Train dataset does not exist !\")\n",
        "if not os.path.exists(TEST_DATASET_PATH):\n",
        "    logger.critical(f\"Test dataset does not exist !\")\n",
        "    raise RuntimeError(\"Test dataset does not exist !\")\n",
        "logger.success(\"Datasets are reachable\")\n",
        "\n",
        "\n",
        "GPU_IS_AVAILABLE = torch.cuda.is_available()\n",
        "GPU_COUNT = torch.cuda.device_count()\n",
        "logger.info(f\"{GPU_IS_AVAILABLE=}\")\n",
        "logger.info(f\"{GPU_COUNT=}\")\n",
        "if not GPU_IS_AVAILABLE:\n",
        "    logger.critical(\"GPU and CUDA are not available !\")\n",
        "    raise RuntimeError(\"GPU and CUDA are not available !\")\n",
        "logger.success(\"GPU and CUDA are available\")\n",
        "logger.info(f\"{device=}\")\n",
        "for gpu_id in range(GPU_COUNT):\n",
        "    gpu_name = torch.cuda.get_device_name(0)\n",
        "    logger.info(f\"GPU {gpu_id} : {gpu_name}\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-03T11:47:09.160591Z",
          "iopub.execute_input": "2024-06-03T11:47:09.160904Z",
          "iopub.status.idle": "2024-06-03T11:47:09.17896Z",
          "shell.execute_reply.started": "2024-06-03T11:47:09.160873Z",
          "shell.execute_reply": "2024-06-03T11:47:09.178153Z"
        },
        "trusted": true,
        "id": "n8ZRS4I0u2OL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_train_df = pd.read_csv(TRAIN_DATASET_PATH, index_col=0)\n",
        "logger.success(\"Dataset loaded !\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-03T11:47:09.179976Z",
          "iopub.execute_input": "2024-06-03T11:47:09.180743Z",
          "iopub.status.idle": "2024-06-03T11:47:10.155549Z",
          "shell.execute_reply.started": "2024-06-03T11:47:09.180713Z",
          "shell.execute_reply": "2024-06-03T11:47:10.154615Z"
        },
        "trusted": true,
        "id": "dXfZpdUhu2OL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_train_df.head()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-03T11:47:10.156913Z",
          "iopub.execute_input": "2024-06-03T11:47:10.157253Z",
          "iopub.status.idle": "2024-06-03T11:47:10.169125Z",
          "shell.execute_reply.started": "2024-06-03T11:47:10.157226Z",
          "shell.execute_reply": "2024-06-03T11:47:10.168189Z"
        },
        "trusted": true,
        "id": "mTAmX6w-u2OL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df = pd.read_csv(TEST_DATASET_PATH)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-03T11:47:10.172049Z",
          "iopub.execute_input": "2024-06-03T11:47:10.172418Z",
          "iopub.status.idle": "2024-06-03T11:47:11.060017Z",
          "shell.execute_reply.started": "2024-06-03T11:47:10.172377Z",
          "shell.execute_reply": "2024-06-03T11:47:11.059239Z"
        },
        "trusted": true,
        "id": "jdUQI3kXu2OM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Pre-Processing\n",
        "\n",
        "### Data Cleaning Steps\n",
        "\n",
        "In this section, we performed several data cleaning steps to prepare our dataset for further analysis and modeling. The steps included:\n",
        "\n",
        "1. **Removing Samples with Labels Equal to -1:**\n",
        "   - We removed all rows where the `target` column had a value of -1.\n",
        "\n",
        "2. **Text Cleaning:**\n",
        "   - **Remove HTML Tags:** We used BeautifulSoup to strip HTML tags from the text.\n",
        "   - **Remove URLs:** We used regular expressions to remove URLs from the text.\n",
        "   - **Remove Diacritics:** We used the `unicodedata` library to remove diacritics from the text.\n",
        "   - **Transform to Lowercase:** We converted all text to lowercase.\n",
        "   - **Remove Extra Whitespaces:** We used regular expressions to remove extra whitespaces from the text.\n",
        "\n",
        "3. **Remove NA or Empty Comments:**\n",
        "   - We removed rows where the `comment_text` column was NA or empty after the cleaning steps.\n",
        "\n",
        "The cleaning operations that are always applied include:\n",
        "\n",
        "- Replacing newline characters with spaces\n",
        "- Removing any non-alphanumeric characters (except spaces)\n",
        "- Removing any numbers\n",
        "- Removing any extra spaces\n",
        "- Removing any non-ASCII characters"
      ],
      "metadata": {
        "id": "KRujFCVtu2OM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(all_train_df.iloc[28]['comment_text'])\n",
        "print(\"Toxicity Level: \",all_train_df.iloc[28]['toxic'])\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-03T11:47:11.062136Z",
          "iopub.execute_input": "2024-06-03T11:47:11.062439Z",
          "iopub.status.idle": "2024-06-03T11:47:11.067834Z",
          "shell.execute_reply.started": "2024-06-03T11:47:11.062413Z",
          "shell.execute_reply": "2024-06-03T11:47:11.067001Z"
        },
        "trusted": true,
        "id": "6DNIkfH_u2ON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(all_train_df.iloc[7]['comment_text'])\n",
        "print(\"Toxicity Level: \",all_train_df.iloc[4]['toxic'])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-03T11:47:11.069124Z",
          "iopub.execute_input": "2024-06-03T11:47:11.069635Z",
          "iopub.status.idle": "2024-06-03T11:47:11.077578Z",
          "shell.execute_reply.started": "2024-06-03T11:47:11.06961Z",
          "shell.execute_reply": "2024-06-03T11:47:11.076678Z"
        },
        "trusted": true,
        "id": "EqBqDUWPu2ON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove rows with NaN values in 'comment_text' column\n",
        "all_train_df = all_train_df.dropna(subset=['comment_text'])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-03T11:47:11.078715Z",
          "iopub.execute_input": "2024-06-03T11:47:11.079049Z",
          "iopub.status.idle": "2024-06-03T11:47:11.1186Z",
          "shell.execute_reply.started": "2024-06-03T11:47:11.079016Z",
          "shell.execute_reply": "2024-06-03T11:47:11.117896Z"
        },
        "trusted": true,
        "id": "UY1v1ePtu2ON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def decontracted(phrase):\n",
        "    # specific\n",
        "    phrase = re.sub(r\"won't\", \"will not\", phrase)\n",
        "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
        "\n",
        "    # general\n",
        "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
        "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
        "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
        "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
        "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
        "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
        "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
        "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
        "    return phrase\n",
        "\n",
        "# we are removing the words from the stop words list: 'no', 'nor', 'not'\n",
        "stopwords= ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\",\\\n",
        "            \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', \\\n",
        "            'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their',\\\n",
        "            'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', \\\n",
        "            'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \\\n",
        "            'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', \\\n",
        "            'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after',\\\n",
        "            'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',\\\n",
        "            'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\\\n",
        "            'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', \\\n",
        "            's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', \\\n",
        "            've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn',\\\n",
        "            \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn',\\\n",
        "            \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", \\\n",
        "            'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-03T11:47:11.127982Z",
          "iopub.execute_input": "2024-06-03T11:47:11.128406Z",
          "iopub.status.idle": "2024-06-03T11:47:11.142262Z",
          "shell.execute_reply.started": "2024-06-03T11:47:11.128382Z",
          "shell.execute_reply": "2024-06-03T11:47:11.141461Z"
        },
        "trusted": true,
        "id": "m2D5USn3u2OO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Combining all the above statemennts\n",
        "preprocessed_comments = []\n",
        "# tqdm is for printing the status bar\n",
        "for sentence in tqdm(all_train_df['comment_text'].values):\n",
        "    sent = decontracted(sentence)\n",
        "    sent = sent.replace('\\r', ' ')\n",
        "    sent = sent.replace('\\\"', ' ')\n",
        "    sent = sent.replace('\\n', ' ')\n",
        "    sent = re.sub('[^A-Za-z0-9]+', ' ', sent)\n",
        "    # https://gist.github.com/sebleier/554280\n",
        "    sent = ' '.join(e for e in sent.split() )\n",
        "    preprocessed_comments.append(sent.lower().strip())"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-03T11:47:14.292999Z",
          "iopub.execute_input": "2024-06-03T11:47:14.293383Z",
          "iopub.status.idle": "2024-06-03T11:47:24.779562Z",
          "shell.execute_reply.started": "2024-06-03T11:47:14.293353Z",
          "shell.execute_reply": "2024-06-03T11:47:24.778646Z"
        },
        "trusted": true,
        "id": "L1H2pSLqu2OO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_train_df['comment_text'] = preprocessed_comments\n",
        "print(all_train_df.iloc[1]['comment_text'])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-03T11:47:24.781082Z",
          "iopub.execute_input": "2024-06-03T11:47:24.781621Z",
          "iopub.status.idle": "2024-06-03T11:47:24.805645Z",
          "shell.execute_reply.started": "2024-06-03T11:47:24.781592Z",
          "shell.execute_reply": "2024-06-03T11:47:24.804704Z"
        },
        "trusted": true,
        "id": "P-QIpsHiu2OO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Combining all the above statemennts\n",
        "preprocessed_comments_test = []\n",
        "# tqdm is for printing the status bar\n",
        "for sentence in tqdm(test_df['comment_text'].values):\n",
        "    sent = decontracted(sentence)\n",
        "    sent = sent.replace('\\r', ' ')\n",
        "    sent = sent.replace('\\\"', ' ')\n",
        "    sent = sent.replace('\\n', ' ')\n",
        "    sent = re.sub('[^A-Za-z0-9]+', ' ', sent)\n",
        "    # https://gist.github.com/sebleier/554280\n",
        "    sent = ' '.join(e for e in sent.split())\n",
        "    preprocessed_comments_test.append(sent.lower().strip())"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-03T11:47:34.199733Z",
          "iopub.execute_input": "2024-06-03T11:47:34.200095Z",
          "iopub.status.idle": "2024-06-03T11:47:42.998582Z",
          "shell.execute_reply.started": "2024-06-03T11:47:34.200063Z",
          "shell.execute_reply": "2024-06-03T11:47:42.997689Z"
        },
        "trusted": true,
        "id": "V-Wp9Doiu2OO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df['comment_text'] = preprocessed_comments_test"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-03T11:47:43.000619Z",
          "iopub.execute_input": "2024-06-03T11:47:43.000908Z",
          "iopub.status.idle": "2024-06-03T11:47:43.023147Z",
          "shell.execute_reply.started": "2024-06-03T11:47:43.000883Z",
          "shell.execute_reply": "2024-06-03T11:47:43.022424Z"
        },
        "trusted": true,
        "id": "NBqgSCmiu2OO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Remove samples with labels equal to -1\n",
        "all_train_df = all_train_df[all_train_df['toxic'] != -1]\n",
        "# Step 2: Remove rows with NA or empty comments\n",
        "all_train_df = all_train_df[all_train_df['comment_text'].notna() & (all_train_df['comment_text'].str.strip() != '')]\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-03T11:47:43.024235Z",
          "iopub.execute_input": "2024-06-03T11:47:43.02455Z",
          "iopub.status.idle": "2024-06-03T11:47:43.160303Z",
          "shell.execute_reply.started": "2024-06-03T11:47:43.024523Z",
          "shell.execute_reply": "2024-06-03T11:47:43.159498Z"
        },
        "trusted": true,
        "id": "MXSIouDJu2OO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to clean text\n",
        "def clean_text(text):\n",
        "    cleaned_text = text\n",
        "    # Remove HTML tags\n",
        "    cleaned_text = BeautifulSoup(cleaned_text, \"html.parser\").get_text()\n",
        "    # Remove URLs\n",
        "    cleaned_text = re.sub(r'http\\S+|www\\S+|https\\S+', '', cleaned_text, flags=re.MULTILINE)\n",
        "    # Remove diacritics\n",
        "    cleaned_text = ''.join(c for c in unicodedata.normalize('NFD', cleaned_text) if unicodedata.category(c) != 'Mn')\n",
        "    # Transform to lowercase\n",
        "    cleaned_text = cleaned_text.lower()\n",
        "    # Remove extra whitespaces\n",
        "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text).strip()\n",
        "    return cleaned_text\n",
        "\n",
        "# Apply text cleaning to the 'comment_text' column\n",
        "all_train_df['comment_text'] = all_train_df['comment_text'].apply(clean_text)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-03T11:47:43.161467Z",
          "iopub.execute_input": "2024-06-03T11:47:43.161771Z",
          "iopub.status.idle": "2024-06-03T11:48:16.239769Z",
          "shell.execute_reply.started": "2024-06-03T11:47:43.161746Z",
          "shell.execute_reply": "2024-06-03T11:48:16.238841Z"
        },
        "trusted": true,
        "id": "Bvnn7LDVu2OO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_train_df.head()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-03T11:48:16.240891Z",
          "iopub.execute_input": "2024-06-03T11:48:16.241195Z",
          "iopub.status.idle": "2024-06-03T11:48:16.252975Z",
          "shell.execute_reply.started": "2024-06-03T11:48:16.241169Z",
          "shell.execute_reply": "2024-06-03T11:48:16.252097Z"
        },
        "trusted": true,
        "id": "Xa5Kb-OBu2OO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the cleaned dataset to a new CSV file if needed\n",
        "# all_train_df.to_csv('/kaggle/working/train_data_without_null.csv', index=True)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-03T11:48:16.254203Z",
          "iopub.execute_input": "2024-06-03T11:48:16.254818Z",
          "iopub.status.idle": "2024-06-03T11:48:16.264495Z",
          "shell.execute_reply.started": "2024-06-03T11:48:16.254785Z",
          "shell.execute_reply": "2024-06-03T11:48:16.263685Z"
        },
        "trusted": true,
        "id": "tzagBI7Xu2OP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data analysis"
      ],
      "metadata": {
        "id": "vTV5a9rvu2OP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the distribution of toxicity\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(all_train_df['toxic'], bins=50, kde=True)\n",
        "plt.title('Distribution of Toxicity Scores')\n",
        "plt.xlabel('Toxicity Score')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-03T11:48:16.265654Z",
          "iopub.execute_input": "2024-06-03T11:48:16.266171Z",
          "iopub.status.idle": "2024-06-03T11:48:17.314558Z",
          "shell.execute_reply.started": "2024-06-03T11:48:16.26614Z",
          "shell.execute_reply": "2024-06-03T11:48:17.3137Z"
        },
        "trusted": true,
        "id": "3KJLXnLIu2OP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Update the toxic_label column with boolean values\n",
        "all_train_df['toxic_label'] = (\n",
        "    (all_train_df['toxic'] == 1) |\n",
        "    (all_train_df['severe_toxic'] == 1) |\n",
        "    (all_train_df['obscene'] == 1) |\n",
        "    (all_train_df['threat'] == 1) |\n",
        "    (all_train_df['insult'] == 1) |\n",
        "    (all_train_df['identity_hate'] == 1)\n",
        ").astype(int)\n",
        "\n",
        "# Verify the changes\n",
        "print(all_train_df[['toxic', 'toxic_label']].head())"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-03T11:48:17.318764Z",
          "iopub.execute_input": "2024-06-03T11:48:17.319058Z",
          "iopub.status.idle": "2024-06-03T11:48:17.332817Z",
          "shell.execute_reply.started": "2024-06-03T11:48:17.319032Z",
          "shell.execute_reply": "2024-06-03T11:48:17.331859Z"
        },
        "trusted": true,
        "id": "Gm1mItYpu2OP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the number and percentage of toxic and non-toxic comments\n",
        "toxic_counts = all_train_df['toxic_label'].value_counts()\n",
        "toxic_percentage = all_train_df['toxic_label'].value_counts(normalize=True) * 100\n",
        "\n",
        "# Plot the pie chart\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.pie(toxic_counts, labels=['Non-Toxic', 'Toxic'], autopct=lambda p: f'{p:.1f}% ({int(p * sum(toxic_counts) / 100)})', startangle=140, colors=['skyblue', 'salmon'])\n",
        "plt.title('Percentage and Number of Toxic and Non-Toxic Comments')\n",
        "plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
        "plt.show()\n",
        "\n",
        "# Print the number and percentage values\n",
        "print('Number of non-toxic comments:', toxic_counts[0])\n",
        "print('Number of toxic comments:', toxic_counts[1])\n",
        "print('Percentage of non-toxic comments: {:.2f}%'.format(toxic_percentage[0]))\n",
        "print('Percentage of toxic comments: {:.2f}%'.format(toxic_percentage[1]))\n",
        "print('Total Number: ', all_train_df.shape[0])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-03T11:48:17.334063Z",
          "iopub.execute_input": "2024-06-03T11:48:17.334514Z",
          "iopub.status.idle": "2024-06-03T11:48:17.524118Z",
          "shell.execute_reply.started": "2024-06-03T11:48:17.334482Z",
          "shell.execute_reply": "2024-06-03T11:48:17.522805Z"
        },
        "trusted": true,
        "id": "bA2kiAwmu2OP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the toxicity columns\n",
        "toxicity_columns = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
        "\n",
        "# Check for distribution of different toxicity types\n",
        "toxicity_distributions = all_train_df[toxicity_columns].sum()\n",
        "\n",
        "# Plot the distribution of different toxicity types\n",
        "plt.figure(figsize=(12, 8))\n",
        "toxicity_distributions.plot(kind='bar')\n",
        "plt.title('Distribution of Different Toxicity Types')\n",
        "plt.xlabel('Toxicity Type')\n",
        "plt.ylabel('Count')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n",
        "\n",
        "# Print the number of samples for each toxicity type\n",
        "for toxicity_type in toxicity_columns:\n",
        "    count = (all_train_df[toxicity_type] > 0).sum()\n",
        "    print(f'Number of samples for {toxicity_type}: {count}')\n",
        "\n",
        "# Find the number of comments that belong to only one class\n",
        "exclusive_counts = {}\n",
        "\n",
        "for toxicity_type in toxicity_columns:\n",
        "    mask = (all_train_df[toxicity_type] == 1) & (all_train_df[toxicity_columns].sum(axis=1) == 1)\n",
        "    count = mask.sum()\n",
        "    exclusive_counts[toxicity_type] = count\n",
        "\n",
        "# Print the number of exclusive samples for each toxicity type\n",
        "for toxicity_type, count in exclusive_counts.items():\n",
        "    print(f'Number of comments exclusively for {toxicity_type}: {count}')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-03T11:48:17.529339Z",
          "iopub.execute_input": "2024-06-03T11:48:17.530383Z",
          "iopub.status.idle": "2024-06-03T11:48:18.059915Z",
          "shell.execute_reply.started": "2024-06-03T11:48:17.530334Z",
          "shell.execute_reply": "2024-06-03T11:48:18.058941Z"
        },
        "trusted": true,
        "id": "y709bhfbu2OP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(all_train_df.shape[0])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-03T11:48:18.061347Z",
          "iopub.execute_input": "2024-06-03T11:48:18.061741Z",
          "iopub.status.idle": "2024-06-03T11:48:18.067211Z",
          "shell.execute_reply.started": "2024-06-03T11:48:18.061704Z",
          "shell.execute_reply": "2024-06-03T11:48:18.06631Z"
        },
        "trusted": true,
        "id": "Mk72cpp_u2OP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# all_train_df.to_csv('/kaggle/working/small_data.csv', index=True)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-03T11:48:18.068748Z",
          "iopub.execute_input": "2024-06-03T11:48:18.069174Z",
          "iopub.status.idle": "2024-06-03T11:48:18.077121Z",
          "shell.execute_reply.started": "2024-06-03T11:48:18.069135Z",
          "shell.execute_reply": "2024-06-03T11:48:18.076394Z"
        },
        "trusted": true,
        "id": "KZ7lw370u2OP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Augmentation\n",
        "\n",
        "Now, let's perform the data augmentations step by step:\n",
        "\n",
        "#### 1. Unique Words Augmentation"
      ],
      "metadata": {
        "id": "M_GsF7WNu2OQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nlpaug\n",
        "!python3 -m nltk.downloader wordnet\n",
        "!unzip /usr/share/nltk_data/corpora/wordnet.zip -d /usr/share/nltk_data/corpora"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-03T11:48:18.078394Z",
          "iopub.execute_input": "2024-06-03T11:48:18.078714Z",
          "iopub.status.idle": "2024-06-03T11:49:20.30419Z",
          "shell.execute_reply.started": "2024-06-03T11:48:18.078684Z",
          "shell.execute_reply": "2024-06-03T11:49:20.303051Z"
        },
        "trusted": true,
        "id": "-Z3gRACZu2OQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nlpaug.augmenter.word as naw\n",
        "import nlpaug.flow as naf\n",
        "import nlpaug.augmenter.char as nac\n",
        "import nlpaug.augmenter.sentence as nas\n",
        "\n",
        "from nlpaug.util import Action\n",
        "import random"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-03T11:49:25.260337Z",
          "iopub.execute_input": "2024-06-03T11:49:25.260635Z",
          "iopub.status.idle": "2024-06-03T11:49:25.26623Z",
          "shell.execute_reply.started": "2024-06-03T11:49:25.260609Z",
          "shell.execute_reply": "2024-06-03T11:49:25.264991Z"
        },
        "trusted": true,
        "id": "R5fAd1W6u2OQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Synonym Augmentation using WordNet\n",
        "synonym_aug = naw.SynonymAug(aug_src='wordnet')\n",
        "\n",
        "# Contextual Word Embedding Augmentation using BERT for Insertion\n",
        "insert_aug = naw.ContextualWordEmbsAug(model_path='bert-base-uncased', action=\"insert\")\n",
        "\n",
        "# Random Swap Augmentation\n",
        "swap_aug = naw.RandomWordAug(action=\"swap\")\n",
        "\n",
        "# Random Deletion Augmentation\n",
        "delete_aug = naw.RandomWordAug(action=\"delete\")\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-03T11:49:25.564969Z",
          "iopub.execute_input": "2024-06-03T11:49:25.565334Z",
          "iopub.status.idle": "2024-06-03T11:49:25.570921Z",
          "shell.execute_reply.started": "2024-06-03T11:49:25.565306Z",
          "shell.execute_reply": "2024-06-03T11:49:25.569929Z"
        },
        "trusted": true,
        "id": "i6WF13xeu2OQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def unique_words_augmentation(comment):\n",
        "    words = comment.split()\n",
        "    unique_words = list(set(words))\n",
        "    return ' '.join(unique_words)\n",
        "\n",
        "def random_mask(comment, mask_ratio=0.2):\n",
        "    words = comment.split()\n",
        "    num_words_to_mask = int(len(words) * mask_ratio)\n",
        "    mask_indices = random.sample(range(len(words)), num_words_to_mask)\n",
        "    masked_words = [word for idx, word in enumerate(words) if idx not in mask_indices]\n",
        "    return ' '.join(masked_words)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-03T11:49:26.398827Z",
          "iopub.execute_input": "2024-06-03T11:49:26.39996Z",
          "iopub.status.idle": "2024-06-03T11:49:26.406183Z",
          "shell.execute_reply.started": "2024-06-03T11:49:26.399925Z",
          "shell.execute_reply": "2024-06-03T11:49:26.405212Z"
        },
        "trusted": true,
        "id": "tc0_KNlKu2OQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# all_train_df = pd.read_csv(\"/kaggle/working/train_data_cleaned.csv\", index_col=0)\n",
        "# logger.success(\"Dataset loaded !\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-03T11:49:26.895773Z",
          "iopub.execute_input": "2024-06-03T11:49:26.896153Z",
          "iopub.status.idle": "2024-06-03T11:49:26.900105Z",
          "shell.execute_reply.started": "2024-06-03T11:49:26.896121Z",
          "shell.execute_reply": "2024-06-03T11:49:26.899148Z"
        },
        "trusted": true,
        "id": "vtPd9CfGu2OQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_train_df.head()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-03T11:49:27.537741Z",
          "iopub.execute_input": "2024-06-03T11:49:27.538224Z",
          "iopub.status.idle": "2024-06-03T11:49:27.551532Z",
          "shell.execute_reply.started": "2024-06-03T11:49:27.538189Z",
          "shell.execute_reply": "2024-06-03T11:49:27.550564Z"
        },
        "trusted": true,
        "id": "P77in-cuu2Oa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Augment a subset of the data\n",
        "augmented_data = []\n",
        "# Update the toxic_label column with boolean values\n",
        "all_train_df['toxic_label'] = (\n",
        "    (all_train_df['toxic'] == 1) |\n",
        "    (all_train_df['severe_toxic'] == 1) |\n",
        "    (all_train_df['obscene'] == 1) |\n",
        "    (all_train_df['threat'] == 1) |\n",
        "    (all_train_df['insult'] == 1) |\n",
        "    (all_train_df['identity_hate'] == 1)\n",
        ").astype(int)\n",
        "\n",
        "# Define the classes and their respective iteration sizes\n",
        "classes = ['threat', 'identity_hate', 'severe_toxic']\n",
        "iteration_sizes = [2000, 2000, 2000]  # Adjust these sizes based on your needs\n",
        "for idx, class_name in enumerate(classes):\n",
        "    toxic_comments = all_train_df[(all_train_df['toxic_label'] == 1) & (all_train_df[class_name] == 1)]\n",
        "\n",
        "    for i in range(min(iteration_sizes[idx], len(toxic_comments))):  # Adjust based on the iteration size\n",
        "        print(f\"Augmenting {class_name}, Iteration: {i}\")\n",
        "        comment = toxic_comments['comment_text'].iloc[i]\n",
        "        original_row = toxic_comments.iloc[i].to_dict()\n",
        "        # Unique Words Augmentation\n",
        "        unique_comment = unique_words_augmentation(comment)\n",
        "        new_row = original_row.copy()\n",
        "        new_row['comment_text'] = unique_comment\n",
        "        augmented_data.append(new_row)\n",
        "\n",
        "        # Random Mask\n",
        "        masked_comment = random_mask(comment)\n",
        "        new_row = original_row.copy()\n",
        "        new_row['comment_text'] = masked_comment\n",
        "        augmented_data.append(new_row)\n",
        "\n",
        "        augmented_comment = synonym_aug.augment(comment)[0]\n",
        "        new_row = original_row.copy()\n",
        "        new_row['comment_text'] = augmented_comment\n",
        "        augmented_data.append(new_row)\n",
        "\n",
        "        augmented_comment = insert_aug.augment(comment)[0]\n",
        "        new_row = original_row.copy()\n",
        "        new_row['comment_text'] = augmented_comment\n",
        "        augmented_data.append(new_row)\n",
        "\n",
        "        augmented_comment = swap_aug.augment(comment)[0]\n",
        "        new_row = original_row.copy()\n",
        "        new_row['comment_text'] = augmented_comment\n",
        "        augmented_data.append(new_row)\n",
        "\n",
        "        augmented_comment = delete_aug.augment(comment)[0]\n",
        "        new_row = original_row.copy()\n",
        "        new_row['comment_text'] = augmented_comment\n",
        "        augmented_data.append(new_row)\n",
        "\n",
        "# Convert the augmented data to a DataFrame\n",
        "augmented_df = pd.DataFrame(augmented_data)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-03T11:49:28.060788Z",
          "iopub.execute_input": "2024-06-03T11:49:28.061175Z",
          "iopub.status.idle": "2024-06-03T11:49:32.640207Z",
          "shell.execute_reply.started": "2024-06-03T11:49:28.061145Z",
          "shell.execute_reply": "2024-06-03T11:49:32.639025Z"
        },
        "trusted": true,
        "id": "ULUAkF-Eu2Oa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Augment a subset of the data\n",
        "# augmented_data = []\n",
        "\n",
        "# # Filter for toxic comments (target >= threshold)\n",
        "# toxic_comments = all_train_df[all_train_df['target'] >= threshold]\n",
        "\n",
        "# for i in range(5000):  # Adjust this number based on your computational power\n",
        "#     print(f\"In Iteration: {i}\")\n",
        "#     comment = toxic_comments['comment_text'].iloc[i]\n",
        "# #     print(f\"Original comment: {comment}\\n\")\n",
        "\n",
        "#     # Get the original row as a dictionary\n",
        "#     original_row = toxic_comments.iloc[i].to_dict()\n",
        "\n",
        "#     # Unique Words Augmentation\n",
        "#     unique_comment = unique_words_augmentation(comment)\n",
        "#     new_row = original_row.copy()\n",
        "#     new_row['comment_text'] = unique_comment\n",
        "#     augmented_data.append(new_row)\n",
        "# #     print(\"Unique Words Augmentation: \" + unique_comment)\n",
        "\n",
        "#     # Random Mask\n",
        "#     masked_comment = random_mask(comment)\n",
        "#     new_row = original_row.copy()\n",
        "#     new_row['comment_text'] = masked_comment\n",
        "#     augmented_data.append(new_row)\n",
        "# #     print(\"Random Mask Augmentation: \" + masked_comment)\n",
        "\n",
        "#     augmented_comment = synonym_aug.augment(comment)[0]\n",
        "#     new_row = original_row.copy()\n",
        "#     new_row['comment_text'] = augmented_comment\n",
        "#     augmented_data.append(new_row)\n",
        "# #     print(\"Synonym Replacement Augmentation: \" + str(augmented_comment))\n",
        "\n",
        "#     augmented_comment = insert_aug.augment(comment)[0]\n",
        "#     new_row = original_row.copy()\n",
        "#     new_row['comment_text'] = augmented_comment\n",
        "#     augmented_data.append(new_row)\n",
        "# #     print(\"Random Insert Augmentation: \" + str(augmented_comment))\n",
        "\n",
        "#     augmented_comment = swap_aug.augment(comment)[0]\n",
        "#     new_row = original_row.copy()\n",
        "#     new_row['comment_text'] = augmented_comment\n",
        "#     augmented_data.append(new_row)\n",
        "# #     print(\"Random Swap Augmentation: \" + str(augmented_comment))\n",
        "\n",
        "#     augmented_comment = delete_aug.augment(comment)[0]\n",
        "#     new_row = original_row.copy()\n",
        "#     new_row['comment_text'] = augmented_comment\n",
        "#     augmented_data.append(new_row)\n",
        "# #     print(\"Random Delete Augmentation: \" + str(augmented_comment))\n",
        "# #     print(\"\\n\\n\")\n",
        "\n",
        "# augmented_df = pd.DataFrame(augmented_data)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-03T11:49:32.642175Z",
          "iopub.execute_input": "2024-06-03T11:49:32.642516Z",
          "iopub.status.idle": "2024-06-03T11:49:32.649276Z",
          "shell.execute_reply.started": "2024-06-03T11:49:32.642488Z",
          "shell.execute_reply": "2024-06-03T11:49:32.648261Z"
        },
        "trusted": true,
        "id": "fKyhmKcwu2Oa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(augmented_df.shape[0])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-03T11:49:32.650679Z",
          "iopub.execute_input": "2024-06-03T11:49:32.651247Z",
          "iopub.status.idle": "2024-06-03T11:49:32.664115Z",
          "shell.execute_reply.started": "2024-06-03T11:49:32.651213Z",
          "shell.execute_reply": "2024-06-03T11:49:32.663204Z"
        },
        "trusted": true,
        "id": "dosP2gjzu2Ob"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "augmented_df.head()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-03T11:49:32.665812Z",
          "iopub.execute_input": "2024-06-03T11:49:32.666104Z",
          "iopub.status.idle": "2024-06-03T11:49:32.68176Z",
          "shell.execute_reply.started": "2024-06-03T11:49:32.666081Z",
          "shell.execute_reply": "2024-06-03T11:49:32.680775Z"
        },
        "trusted": true,
        "id": "Nccxc1awu2Ob"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine with the original data\n",
        "print(all_train_df.shape[0])\n",
        "print(augmented_df.shape[0])\n",
        "\n",
        "all_train_df = pd.concat([all_train_df, augmented_df], ignore_index=False)\n",
        "print(all_train_df.shape[0])\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-03T11:49:32.682929Z",
          "iopub.execute_input": "2024-06-03T11:49:32.683311Z",
          "iopub.status.idle": "2024-06-03T11:49:32.69891Z",
          "shell.execute_reply.started": "2024-06-03T11:49:32.683279Z",
          "shell.execute_reply": "2024-06-03T11:49:32.698079Z"
        },
        "trusted": true,
        "id": "Euhk_K0Eu2Ob"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_train_df.head()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-03T11:49:32.731945Z",
          "iopub.execute_input": "2024-06-03T11:49:32.732241Z",
          "iopub.status.idle": "2024-06-03T11:49:32.743884Z",
          "shell.execute_reply.started": "2024-06-03T11:49:32.732218Z",
          "shell.execute_reply": "2024-06-03T11:49:32.74304Z"
        },
        "trusted": true,
        "id": "1xLc-oDWu2Ob"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# all_train_df.iloc[159576]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-03T11:49:37.755154Z",
          "iopub.execute_input": "2024-06-03T11:49:37.755831Z",
          "iopub.status.idle": "2024-06-03T11:49:37.759661Z",
          "shell.execute_reply.started": "2024-06-03T11:49:37.755796Z",
          "shell.execute_reply": "2024-06-03T11:49:37.758701Z"
        },
        "trusted": true,
        "id": "3zhdPnClu2Ob"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_train_df.to_csv('/kaggle/working/train_data_augumented_by_21k.csv', index=True)\n",
        "# all_train_df = pd.read_csv(\"/kaggle/input/dataaugumented-cleaned/train_data_augumented_by_21k.csv\")\n",
        "# logger.success(\"Dataset loaded !\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-03T11:49:38.308026Z",
          "iopub.execute_input": "2024-06-03T11:49:38.308857Z",
          "iopub.status.idle": "2024-06-03T11:49:41.423364Z",
          "shell.execute_reply.started": "2024-06-03T11:49:38.308822Z",
          "shell.execute_reply": "2024-06-03T11:49:41.422383Z"
        },
        "trusted": true,
        "id": "I4CDF3xGu2Ob"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for distribution of different toxicity types\n",
        "toxicity_columns = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
        "toxicity_distributions = all_train_df[toxicity_columns].sum()\n",
        "\n",
        "# Plot the distribution of different toxicity types\n",
        "plt.figure(figsize=(12, 8))\n",
        "toxicity_distributions.plot(kind='bar')\n",
        "plt.title('Distribution of Different Toxicity Types')\n",
        "plt.xlabel('Toxicity Type')\n",
        "plt.ylabel('Count')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n",
        "\n",
        "# Print the number of samples for each toxicity type\n",
        "for toxicity_type in toxicity_columns:\n",
        "    count = (all_train_df[toxicity_type] > 0).sum()\n",
        "    print(f'Number of samples for {toxicity_type}: {count}')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-03T11:49:47.821615Z",
          "iopub.execute_input": "2024-06-03T11:49:47.822062Z",
          "iopub.status.idle": "2024-06-03T11:49:48.147799Z",
          "shell.execute_reply.started": "2024-06-03T11:49:47.822027Z",
          "shell.execute_reply": "2024-06-03T11:49:48.146795Z"
        },
        "trusted": true,
        "id": "If8L-cGPu2Ob"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the number and percentage of toxic and non-toxic comments\n",
        "toxic_counts = all_train_df['toxic_label'].value_counts()\n",
        "toxic_percentage = all_train_df['toxic_label'].value_counts(normalize=True) * 100\n",
        "\n",
        "# Plot the pie chart\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.pie(toxic_counts, labels=['Non-Toxic', 'Toxic'], autopct=lambda p: f'{p:.1f}% ({int(p * sum(toxic_counts) / 100)})', startangle=140, colors=['skyblue', 'salmon'])\n",
        "plt.title('Percentage and Number of Toxic and Non-Toxic Comments')\n",
        "plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
        "plt.show()\n",
        "\n",
        "# Print the number and percentage values\n",
        "print('Number of non-toxic comments:', toxic_counts[0])\n",
        "print('Number of toxic comments:', toxic_counts[1])\n",
        "print('Percentage of non-toxic comments: {:.2f}%'.format(toxic_percentage[0]))\n",
        "print('Percentage of toxic comments: {:.2f}%'.format(toxic_percentage[1]))\n",
        "print('Total Number: ', all_train_df.shape[0])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-03T11:49:59.842274Z",
          "iopub.execute_input": "2024-06-03T11:49:59.843028Z",
          "iopub.status.idle": "2024-06-03T11:50:00.024558Z",
          "shell.execute_reply.started": "2024-06-03T11:49:59.842983Z",
          "shell.execute_reply": "2024-06-03T11:50:00.023013Z"
        },
        "trusted": true,
        "id": "AjeefFybu2Ob"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_train_df.head()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-03T11:50:00.098692Z",
          "iopub.execute_input": "2024-06-03T11:50:00.098999Z",
          "iopub.status.idle": "2024-06-03T11:50:00.111397Z",
          "shell.execute_reply.started": "2024-06-03T11:50:00.098962Z",
          "shell.execute_reply": "2024-06-03T11:50:00.110425Z"
        },
        "trusted": true,
        "id": "BgpVY4Yfu2Ob"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset\n"
      ],
      "metadata": {
        "id": "_8ICiKTxu2Oc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = all_train_df[~all_train_df.isna()]\n",
        "# removing sample with labels equal to -1\n",
        "train_df = train_df.loc[train_df['toxic'] >= 0]\n",
        "train_df.reset_index(inplace=True)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-03T11:50:00.881559Z",
          "iopub.execute_input": "2024-06-03T11:50:00.882243Z",
          "iopub.status.idle": "2024-06-03T11:50:00.992873Z",
          "shell.execute_reply.started": "2024-06-03T11:50:00.882208Z",
          "shell.execute_reply": "2024-06-03T11:50:00.99211Z"
        },
        "trusted": true,
        "id": "a4J5n-LSu2Oc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample 10,000 rows for validation\n",
        "validation_df = train_df.sample(n=10_000, random_state=42)\n",
        "\n",
        "# Remove the validation samples from the training dataset\n",
        "train_df = train_df.drop(validation_df.index)\n",
        "\n",
        "train_df[LABEL_LIST] = (train_df[LABEL_LIST]>=0.5).astype(int)\n",
        "validation_df[LABEL_LIST] = (validation_df[LABEL_LIST]>=0.5).astype(int)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-03T11:50:01.188696Z",
          "iopub.execute_input": "2024-06-03T11:50:01.18944Z",
          "iopub.status.idle": "2024-06-03T11:50:01.235792Z",
          "shell.execute_reply.started": "2024-06-03T11:50:01.189407Z",
          "shell.execute_reply": "2024-06-03T11:50:01.235011Z"
        },
        "trusted": true,
        "id": "dJ9qrSg3u2Oc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.head()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-03T11:50:01.600781Z",
          "iopub.execute_input": "2024-06-03T11:50:01.601607Z",
          "iopub.status.idle": "2024-06-03T11:50:01.614712Z",
          "shell.execute_reply.started": "2024-06-03T11:50:01.601575Z",
          "shell.execute_reply": "2024-06-03T11:50:01.613844Z"
        },
        "trusted": true,
        "id": "p0k3Owo6u2Oc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.shape[0]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-03T11:50:02.263752Z",
          "iopub.execute_input": "2024-06-03T11:50:02.264109Z",
          "iopub.status.idle": "2024-06-03T11:50:02.2699Z",
          "shell.execute_reply.started": "2024-06-03T11:50:02.264081Z",
          "shell.execute_reply": "2024-06-03T11:50:02.268965Z"
        },
        "trusted": true,
        "id": "tk5EvQo6u2Oc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "validation_df.shape[0]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-03T11:50:02.563625Z",
          "iopub.execute_input": "2024-06-03T11:50:02.563974Z",
          "iopub.status.idle": "2024-06-03T11:50:02.569801Z",
          "shell.execute_reply.started": "2024-06-03T11:50:02.563948Z",
          "shell.execute_reply": "2024-06-03T11:50:02.568896Z"
        },
        "trusted": true,
        "id": "tHRXp0G2u2Oc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class JigsawDataset(Dataset):\n",
        "    def __init__(self, data_df, tokenizer):\n",
        "        self.data = data_df\n",
        "        self.tokenizer = tokenizer\n",
        "        self.labels_present = all(label in data_df.columns for label in LABEL_LIST)\n",
        "        if not self.labels_present:\n",
        "            # Add columns for labels if not present\n",
        "            for label in LABEL_LIST:\n",
        "                self.data[label] = 0\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        comment = self.data.iloc[index][\"comment_text\"]\n",
        "        label = torch.tensor(self.data.iloc[index][LABEL_LIST].tolist(), dtype=torch.float)\n",
        "\n",
        "        token_list, attention_mask = self.text_to_token_and_mask(comment)\n",
        "\n",
        "        return dict(index=index, ids=token_list, mask=attention_mask, labels=label)\n",
        "\n",
        "    def text_to_token_and_mask(self, input_text):\n",
        "        tokenization_dict = tokenizer.encode_plus(input_text,\n",
        "                                add_special_tokens=True,\n",
        "                                max_length=128,\n",
        "                                padding='max_length',\n",
        "                                truncation=True,\n",
        "                                return_attention_mask=True,\n",
        "                                return_tensors='pt')\n",
        "        token_list = tokenization_dict[\"input_ids\"].flatten()\n",
        "        attention_mask = tokenization_dict[\"attention_mask\"].flatten()\n",
        "        return (token_list, attention_mask)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-03T11:50:02.826756Z",
          "iopub.execute_input": "2024-06-03T11:50:02.82763Z",
          "iopub.status.idle": "2024-06-03T11:50:02.838735Z",
          "shell.execute_reply.started": "2024-06-03T11:50:02.827595Z",
          "shell.execute_reply": "2024-06-03T11:50:02.837889Z"
        },
        "trusted": true,
        "id": "O63BDGrdu2Oc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def set_lr(optim, lr):\n",
        "    '''\n",
        "    Set the learning rate in the optimizer\n",
        "    '''\n",
        "    for g in optim.param_groups:\n",
        "        g['lr'] = lr\n",
        "    return optim"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-03T11:50:03.222529Z",
          "iopub.execute_input": "2024-06-03T11:50:03.22289Z",
          "iopub.status.idle": "2024-06-03T11:50:03.228158Z",
          "shell.execute_reply.started": "2024-06-03T11:50:03.222859Z",
          "shell.execute_reply": "2024-06-03T11:50:03.22715Z"
        },
        "trusted": true,
        "id": "VXm9mKkTu2Oc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transformer class and functions for models and predictions\n",
        "\n",
        "class TransformerClassifierStack(nn.Module):\n",
        "    def __init__(self, tr_model, nb_labels, dropout_prob=0.4, freeze=False):\n",
        "        super().__init__()\n",
        "        self.tr_model = tr_model\n",
        "\n",
        "        # Stack features of 4 last encoders\n",
        "        self.hidden_dim = tr_model.config.hidden_size * 4\n",
        "\n",
        "        # hidden linear for the classification\n",
        "        self.dropout = nn.Dropout(dropout_prob)\n",
        "        self.hl = nn.Linear(self.hidden_dim, self.hidden_dim)\n",
        "\n",
        "        # Last Linear for the classification\n",
        "        self.last_l = nn.Linear(self.hidden_dim, nb_labels)\n",
        "\n",
        "        # freeze all the parameters if necessary\n",
        "        for param in self.tr_model.parameters():\n",
        "            param.requires_grad = not freeze\n",
        "\n",
        "        # init learning params of last layers\n",
        "        torch.nn.init.xavier_uniform_(self.hl.weight)\n",
        "        torch.nn.init.xavier_uniform_(self.last_l.weight)\n",
        "\n",
        "    def forward(self, ids, mask):\n",
        "        # ids = [batch_size, padded_seq_len]\n",
        "        # mask = [batch_size, padded_seq_len]\n",
        "        # mask: avoid to make self attention on padded data\n",
        "        tr_output = self.tr_model(input_ids=ids,\n",
        "                                  attention_mask=mask,\n",
        "                                  output_hidden_states=True)\n",
        "\n",
        "        # Get all the hidden states\n",
        "        hidden_states = tr_output['hidden_states']\n",
        "\n",
        "        # hs_* = [batch_size, padded_seq_len, 768]\n",
        "        hs_1 = hidden_states[-1][:, 0, :]\n",
        "        hs_2 = hidden_states[-2][:, 0, :]\n",
        "        hs_3 = hidden_states[-3][:, 0, :]\n",
        "        hs_4 = hidden_states[-4][:, 0, :]\n",
        "\n",
        "        # features_vec = [batch_size, 768 * 4]\n",
        "        features_vec = torch.cat([hs_1, hs_2, hs_3, hs_4], dim=-1)\n",
        "\n",
        "        x = self.dropout(features_vec)\n",
        "        x = self.hl(x)\n",
        "\n",
        "        # x = [batch_size, 768 * 4]\n",
        "        x = torch.tanh(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.last_l(x)\n",
        "\n",
        "        # x = [batch_size, 1]\n",
        "        return x\n",
        "\n",
        "def load_roberta_model(nb_labels):\n",
        "    '''\n",
        "    Load RoBERTa model without any checkpoint\n",
        "    RoBERTa for finetuning\n",
        "    '''\n",
        "    logger.info(f\"transformers.RobertaTokenizer : roberta-base\")\n",
        "    logger.info(f\"transformers.AutoModel : roberta-base\")\n",
        "    tokenizer = transformers.RobertaTokenizer.from_pretrained('roberta-base')\n",
        "    tr_model = transformers.AutoModel.from_pretrained('roberta-base')\n",
        "    model = TransformerClassifierStack(tr_model, nb_labels, freeze=True)\n",
        "    return model, tokenizer\n",
        "\n",
        "\n",
        "def load_roberta_pretrained(path, nb_labels, lr=2e-5):\n",
        "    '''\n",
        "    Load RoBERTa from checkout point (already trained on Hate Speech tasks)\n",
        "    '''\n",
        "    tokenizer = transformers.RobertaTokenizer.from_pretrained('roberta-base')\n",
        "    tr_model = transformers.AutoModel.from_pretrained('roberta-base')\n",
        "    model = TransformerClassifierStack(tr_model, nb_labels)\n",
        "\n",
        "    loaded = torch.load(path)\n",
        "    model.load_state_dict(loaded['state_dict'])\n",
        "\n",
        "    optimizer = transformers.AdamW(model.parameters(), lr=lr)\n",
        "    optimizer.load_state_dict(loaded['optimizer_dict'])\n",
        "    optimizer = set_lr(optimizer, lr)\n",
        "\n",
        "    return model, tokenizer, optimizer\n",
        "\n",
        "def preds_fn(batch, model, device):\n",
        "    '''\n",
        "    Get the predictions for one batch according to the model\n",
        "    '''\n",
        "    b_input = batch['ids'].to(device)\n",
        "    b_mask = batch['mask'].to(device)\n",
        "\n",
        "    return model(b_input, b_mask)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-03T11:50:03.546657Z",
          "iopub.execute_input": "2024-06-03T11:50:03.546976Z",
          "iopub.status.idle": "2024-06-03T11:50:03.562707Z",
          "shell.execute_reply.started": "2024-06-03T11:50:03.546951Z",
          "shell.execute_reply": "2024-06-03T11:50:03.561762Z"
        },
        "trusted": true,
        "id": "qcHBsiZCu2Oc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the model\n",
        "model, tokenizer = load_roberta_model(nb_labels=len(LABEL_LIST))\n",
        "logger.success(\"Model loaded !\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-03T11:50:03.779477Z",
          "iopub.execute_input": "2024-06-03T11:50:03.780188Z",
          "iopub.status.idle": "2024-06-03T11:50:08.440943Z",
          "shell.execute_reply.started": "2024-06-03T11:50:03.780152Z",
          "shell.execute_reply": "2024-06-03T11:50:08.440063Z"
        },
        "trusted": true,
        "id": "T6fCsSdou2Od"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 32\n",
        "LR=1e-4\n",
        "PIN_MEMORY = True\n",
        "NUM_WORKERS = 0\n",
        "PREFETCH_FACTOR = \"None\"\n",
        "NUM_EPOCHS = 3\n",
        "logger.info(f\"{BATCH_SIZE=}\")\n",
        "logger.info(f\"{LR=}\")\n",
        "logger.info(f\"{PIN_MEMORY=}\")\n",
        "logger.info(f\"{NUM_WORKERS=}\")\n",
        "logger.info(f\"{PREFETCH_FACTOR=}\")\n",
        "logger.info(f\"{NUM_EPOCHS=}\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-03T11:50:08.442448Z",
          "iopub.execute_input": "2024-06-03T11:50:08.442758Z",
          "iopub.status.idle": "2024-06-03T11:50:08.456275Z",
          "shell.execute_reply.started": "2024-06-03T11:50:08.44273Z",
          "shell.execute_reply": "2024-06-03T11:50:08.455388Z"
        },
        "trusted": true,
        "id": "DTL_55uPu2Od"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Losses**"
      ],
      "metadata": {
        "id": "4NN0ysi1u2Od"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self,\n",
        "                 gamma: float = 2,\n",
        "                 reduction: str = \"mean\",\n",
        "                 pos_weight: torch.Tensor = None):\n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.gamma= gamma\n",
        "        self.reduction = reduction\n",
        "        self.pos_weight = pos_weight\n",
        "\n",
        "    def forward(self, inputs: torch.Tensor,\n",
        "                targets: torch.Tensor):\n",
        "        p = torch.sigmoid(inputs)\n",
        "        ce_loss = F.binary_cross_entropy_with_logits(\n",
        "            inputs, targets, reduction=\"none\", pos_weight=self.pos_weight\n",
        "        )\n",
        "        p_t =  p * targets + (1 - p) * (1 - targets)\n",
        "        loss = ce_loss * ((1 - p_t) ** self.gamma)\n",
        "\n",
        "        if self.reduction == \"mean\":\n",
        "            loss = loss.mean()\n",
        "        elif self.reduction == \"sum\":\n",
        "            loss = loss.sum()\n",
        "\n",
        "        return loss"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-03T11:50:08.457349Z",
          "iopub.execute_input": "2024-06-03T11:50:08.457616Z",
          "iopub.status.idle": "2024-06-03T11:50:08.465743Z",
          "shell.execute_reply.started": "2024-06-03T11:50:08.457593Z",
          "shell.execute_reply": "2024-06-03T11:50:08.464878Z"
        },
        "trusted": true,
        "id": "yhVPKmx2u2Od"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class ResampleLoss(nn.Module):\n",
        "\n",
        "    def __init__(self,\n",
        "                 use_sigmoid=True, partial=False,\n",
        "                 loss_weight=1.0, reduction='mean',\n",
        "                 reweight_func=None,  # None, 'inv', 'sqrt_inv', 'rebalance', 'CB'\n",
        "                 weight_norm=None, # None, 'by_instance', 'by_batch'\n",
        "                 focal=dict(\n",
        "                     focal=True,\n",
        "                     alpha=0.5,\n",
        "                     gamma=2,\n",
        "                 ),\n",
        "                 map_param=dict(\n",
        "                     alpha=10.0,\n",
        "                     beta=0.2,\n",
        "                     gamma=0.1\n",
        "                 ),\n",
        "                 CB_loss=dict(\n",
        "                     CB_beta=0.9,\n",
        "                     CB_mode='average_w'  # 'by_class', 'average_n', 'average_w', 'min_n'\n",
        "                 ),\n",
        "                 logit_reg=dict(\n",
        "                     neg_scale=5.0,\n",
        "                     init_bias=0.1\n",
        "                 ),\n",
        "                 class_freq=None,\n",
        "                 train_num=None):\n",
        "        super(ResampleLoss, self).__init__()\n",
        "\n",
        "        assert (use_sigmoid is True) or (partial is False)\n",
        "        self.use_sigmoid = use_sigmoid\n",
        "        self.partial = partial\n",
        "        self.loss_weight = loss_weight\n",
        "        self.reduction = reduction\n",
        "        if self.use_sigmoid:\n",
        "            if self.partial:\n",
        "                raise RuntimeError(\"Not defined here\")\n",
        "                self.cls_criterion = partial_cross_entropy\n",
        "            else:\n",
        "                self.cls_criterion = binary_cross_entropy\n",
        "        else:\n",
        "            raise RuntimeError(\"Not defined here\")\n",
        "            self.cls_criterion = cross_entropy\n",
        "\n",
        "        # reweighting function\n",
        "        self.reweight_func = reweight_func\n",
        "\n",
        "        # normalization (optional)\n",
        "        self.weight_norm = weight_norm\n",
        "\n",
        "        # focal loss params\n",
        "        self.focal = focal['focal']\n",
        "        self.gamma = focal['gamma']\n",
        "        self.alpha = focal['alpha'] # change to alpha\n",
        "\n",
        "        # mapping function params\n",
        "        self.map_alpha = map_param['alpha']\n",
        "        self.map_beta = map_param['beta']\n",
        "        self.map_gamma = map_param['gamma']\n",
        "\n",
        "        # CB loss params (optional)\n",
        "        self.CB_beta = CB_loss['CB_beta']\n",
        "        self.CB_mode = CB_loss['CB_mode']\n",
        "\n",
        "        self.class_freq = torch.from_numpy(np.asarray(class_freq)).float().cuda()\n",
        "        self.num_classes = self.class_freq.shape[0]\n",
        "        self.train_num = train_num # only used to be divided by class_freq\n",
        "        # regularization params\n",
        "        self.logit_reg = logit_reg\n",
        "        self.neg_scale = logit_reg[\n",
        "            'neg_scale'] if 'neg_scale' in logit_reg else 1.0\n",
        "        init_bias = logit_reg['init_bias'] if 'init_bias' in logit_reg else 0.0\n",
        "\n",
        "        self.init_bias = - torch.log(\n",
        "            self.train_num / self.class_freq - 1) * init_bias\n",
        "\n",
        "        self.freq_inv = torch.ones(self.class_freq.shape).cuda() / self.class_freq\n",
        "        self.propotion_inv = self.train_num / self.class_freq\n",
        "\n",
        "    def forward(self,\n",
        "                cls_score,\n",
        "                label,\n",
        "                weight=None,\n",
        "                avg_factor=None,\n",
        "                reduction_override=None,\n",
        "                **kwargs):\n",
        "\n",
        "        assert reduction_override in (None, 'none', 'mean', 'sum')\n",
        "        reduction = (\n",
        "            reduction_override if reduction_override else self.reduction)\n",
        "\n",
        "        weight = self.reweight_functions(label)\n",
        "\n",
        "        cls_score, weight = self.logit_reg_functions(label.float(), cls_score, weight)\n",
        "\n",
        "        if self.focal:\n",
        "            logpt = self.cls_criterion(\n",
        "                cls_score.clone(), label, weight=None, reduction='none',\n",
        "                avg_factor=avg_factor)\n",
        "            # pt is sigmoid(logit) for pos or sigmoid(-logit) for neg\n",
        "            pt = torch.exp(-logpt)\n",
        "            wtloss = self.cls_criterion(\n",
        "                cls_score, label.float(), weight=weight, reduction='none')\n",
        "            alpha_t = torch.where(label==1, self.alpha, 1-self.alpha)\n",
        "            loss = alpha_t * ((1 - pt) ** self.gamma) * wtloss # balance_param should be a tensor\n",
        "            loss = reduce_loss(loss, reduction)             # add reduction\n",
        "        else:\n",
        "            loss = self.cls_criterion(cls_score, label.float(), weight,\n",
        "                                      reduction=reduction)\n",
        "\n",
        "        loss = self.loss_weight * loss\n",
        "        return loss\n",
        "\n",
        "    def reweight_functions(self, label):\n",
        "        if self.reweight_func is None:\n",
        "            return None\n",
        "        elif self.reweight_func in ['inv', 'sqrt_inv']:\n",
        "            weight = self.RW_weight(label.float())\n",
        "        elif self.reweight_func in 'rebalance':\n",
        "            weight = self.rebalance_weight(label.float())\n",
        "        elif self.reweight_func in 'CB':\n",
        "            weight = self.CB_weight(label.float())\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "        if self.weight_norm is not None:\n",
        "            if 'by_instance' in self.weight_norm:\n",
        "                max_by_instance, _ = torch.max(weight, dim=-1, keepdim=True)\n",
        "                weight = weight / max_by_instance\n",
        "            elif 'by_batch' in self.weight_norm:\n",
        "                weight = weight / torch.max(weight)\n",
        "\n",
        "        return weight\n",
        "\n",
        "    def logit_reg_functions(self, labels, logits, weight=None):\n",
        "        if not self.logit_reg:\n",
        "            return logits, weight\n",
        "        if 'init_bias' in self.logit_reg:\n",
        "            logits += self.init_bias\n",
        "        if 'neg_scale' in self.logit_reg:\n",
        "            logits = logits * (1 - labels) * self.neg_scale  + logits * labels\n",
        "            if weight is not None:\n",
        "                weight = weight / self.neg_scale * (1 - labels) + weight * labels\n",
        "        return logits, weight\n",
        "\n",
        "    def rebalance_weight(self, gt_labels):\n",
        "        repeat_rate = torch.sum( gt_labels.float() * self.freq_inv, dim=1, keepdim=True)\n",
        "        pos_weight = self.freq_inv.clone().detach().unsqueeze(0) / repeat_rate\n",
        "        # pos and neg are equally treated\n",
        "        weight = torch.sigmoid(self.map_beta * (pos_weight - self.map_gamma)) + self.map_alpha\n",
        "        return weight\n",
        "\n",
        "    def CB_weight(self, gt_labels):\n",
        "        if  'by_class' in self.CB_mode:\n",
        "            weight = torch.tensor((1 - self.CB_beta)).cuda() / \\\n",
        "                     (1 - torch.pow(self.CB_beta, self.class_freq)).cuda()\n",
        "        elif 'average_n' in self.CB_mode:\n",
        "            avg_n = torch.sum(gt_labels * self.class_freq, dim=1, keepdim=True) / \\\n",
        "                    torch.sum(gt_labels, dim=1, keepdim=True)\n",
        "            weight = torch.tensor((1 - self.CB_beta)).cuda() / \\\n",
        "                     (1 - torch.pow(self.CB_beta, avg_n)).cuda()\n",
        "        elif 'average_w' in self.CB_mode:\n",
        "            weight_ = torch.tensor((1 - self.CB_beta)).cuda() / \\\n",
        "                      (1 - torch.pow(self.CB_beta, self.class_freq)).cuda()\n",
        "            weight = torch.sum(gt_labels * weight_, dim=1, keepdim=True) / \\\n",
        "                     torch.sum(gt_labels, dim=1, keepdim=True)\n",
        "        elif 'min_n' in self.CB_mode:\n",
        "            min_n, _ = torch.min(gt_labels * self.class_freq +\n",
        "                                 (1 - gt_labels) * 100000, dim=1, keepdim=True)\n",
        "            weight = torch.tensor((1 - self.CB_beta)).cuda() / \\\n",
        "                     (1 - torch.pow(self.CB_beta, min_n)).cuda()\n",
        "        else:\n",
        "            raise NameError\n",
        "        return weight\n",
        "\n",
        "    def RW_weight(self, gt_labels, by_class=True):\n",
        "        if 'sqrt' in self.reweight_func:\n",
        "            weight = torch.sqrt(self.propotion_inv)\n",
        "        else:\n",
        "            weight = self.propotion_inv\n",
        "        if not by_class:\n",
        "            sum_ = torch.sum(weight * gt_labels, dim=1, keepdim=True)\n",
        "            weight = sum_ / torch.sum(gt_labels, dim=1, keepdim=True)\n",
        "        return weight\n",
        "\n",
        "\n",
        "def reduce_loss(loss, reduction):\n",
        "    \"\"\"Reduce loss as specified.\n",
        "    Args:\n",
        "        loss (Tensor): Elementwise loss tensor.\n",
        "        reduction (str): Options are \"none\", \"mean\" and \"sum\".\n",
        "    Return:\n",
        "        Tensor: Reduced loss tensor.\n",
        "    \"\"\"\n",
        "    reduction_enum = F._Reduction.get_enum(reduction)\n",
        "    # none: 0, elementwise_mean:1, sum: 2\n",
        "    if reduction_enum == 0:\n",
        "        return loss\n",
        "    elif reduction_enum == 1:\n",
        "        return loss.mean()\n",
        "    elif reduction_enum == 2:\n",
        "        return loss.sum()\n",
        "\n",
        "\n",
        "def weight_reduce_loss(loss, weight=None, reduction='mean', avg_factor=None):\n",
        "    \"\"\"Apply element-wise weight and reduce loss.\n",
        "    Args:\n",
        "        loss (Tensor): Element-wise loss.\n",
        "        weight (Tensor): Element-wise weights.\n",
        "        reduction (str): Same as built-in losses of PyTorch.\n",
        "        avg_factor (float): Avarage factor when computing the mean of losses.\n",
        "    Returns:\n",
        "        Tensor: Processed loss values.\n",
        "    \"\"\"\n",
        "    # if weight is specified, apply element-wise weight\n",
        "    if weight is not None:\n",
        "        loss = loss * weight\n",
        "\n",
        "    # if avg_factor is not specified, just reduce the loss\n",
        "    if avg_factor is None:\n",
        "        loss = reduce_loss(loss, reduction)\n",
        "    else:\n",
        "        # if reduction is mean, then average the loss by avg_factor\n",
        "        if reduction == 'mean':\n",
        "            loss = loss.sum() / avg_factor\n",
        "        # if reduction is 'none', then do nothing, otherwise raise an error\n",
        "        elif reduction != 'none':\n",
        "            raise ValueError('avg_factor can not be used with reduction=\"sum\"')\n",
        "    return loss\n",
        "\n",
        "\n",
        "def binary_cross_entropy(pred,\n",
        "                         label,\n",
        "                         weight=None,\n",
        "                         reduction='mean',\n",
        "                         avg_factor=None):\n",
        "\n",
        "    # weighted element-wise losses\n",
        "    if weight is not None:\n",
        "        weight = weight.float()\n",
        "\n",
        "    loss = F.binary_cross_entropy_with_logits(\n",
        "        pred, label.float(), weight, reduction='none')\n",
        "    loss = weight_reduce_loss(loss, reduction=reduction, avg_factor=avg_factor)\n",
        "\n",
        "    return loss"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-03T11:50:08.467857Z",
          "iopub.execute_input": "2024-06-03T11:50:08.46836Z",
          "iopub.status.idle": "2024-06-03T11:50:08.507435Z",
          "shell.execute_reply.started": "2024-06-03T11:50:08.468328Z",
          "shell.execute_reply": "2024-06-03T11:50:08.50655Z"
        },
        "trusted": true,
        "id": "-vha-Dt6u2Od"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_label_weights_bce(df, classes=LABEL_LIST):\n",
        "    weights = torch.empty((len(classes),))\n",
        "\n",
        "    nb_samples = len(df)\n",
        "\n",
        "    for idx, c in enumerate(classes):\n",
        "        nb_zeros = len(df[df[c] == 0])\n",
        "        nb_ones = nb_samples - nb_zeros\n",
        "        weights[idx] = nb_zeros / nb_ones\n",
        "\n",
        "    return weights\n",
        "\n",
        "def get_label_inv_freq(df, classes=LABEL_LIST):\n",
        "    weights = torch.empty((len(classes),))\n",
        "    nb_samples = len(df)\n",
        "\n",
        "    for idx, c in enumerate(classes):\n",
        "        nb_zeros = len(df[df[c] == 0])\n",
        "        weights[idx] = (nb_zeros / nb_samples)\n",
        "\n",
        "    return weights\n",
        "\n",
        "def get_nb_samples_lab(df, classes=LABEL_LIST):\n",
        "    nb_ones_tot, nb_zeros_tot = [], []\n",
        "    nb_tot = len(df)\n",
        "\n",
        "    for c in classes:\n",
        "        nb_zeros = len(df[df[c] == 0])\n",
        "        nb_ones = nb_tot - nb_zeros\n",
        "\n",
        "        nb_ones_tot.append(nb_ones)\n",
        "        nb_zeros_tot.append(nb_zeros)\n",
        "\n",
        "    return torch.tensor(nb_ones_tot), torch.tensor(nb_zeros_tot)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-03T11:50:08.508523Z",
          "iopub.execute_input": "2024-06-03T11:50:08.508845Z",
          "iopub.status.idle": "2024-06-03T11:50:08.522792Z",
          "shell.execute_reply.started": "2024-06-03T11:50:08.508813Z",
          "shell.execute_reply": "2024-06-03T11:50:08.521813Z"
        },
        "trusted": true,
        "id": "6cnsFTW1u2Od"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data and Data Loaders"
      ],
      "metadata": {
        "id": "uN3v104pu2Od"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = JigsawDataset(train_df, tokenizer)\n",
        "train_dataloader = DataLoader(train_dataset,\n",
        "                             batch_size=BATCH_SIZE,\n",
        "                             shuffle=True,\n",
        "                             num_workers=NUM_WORKERS,\n",
        "                             pin_memory=PIN_MEMORY)\n",
        "\n",
        "validation_dataset = JigsawDataset(validation_df, tokenizer)\n",
        "validation_dataloader = DataLoader(validation_dataset,\n",
        "                             batch_size=BATCH_SIZE,\n",
        "                             shuffle=True,\n",
        "                             num_workers=NUM_WORKERS,\n",
        "                             pin_memory=PIN_MEMORY)\n",
        "\n",
        "\n",
        "criterion = FocalLoss()\n",
        "logger.info(criterion)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n",
        "logger.info(optimizer)\n",
        "\n",
        "model.to(device)\n",
        "criterion.to(device)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-03T11:50:09.020296Z",
          "iopub.execute_input": "2024-06-03T11:50:09.021057Z",
          "iopub.status.idle": "2024-06-03T11:50:09.316141Z",
          "shell.execute_reply.started": "2024-06-03T11:50:09.021022Z",
          "shell.execute_reply": "2024-06-03T11:50:09.315176Z"
        },
        "trusted": true,
        "id": "DTRycO0Eu2Od"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class HammingLossWithoutThreshold(Metric):\n",
        "    def __init__(self, num_classes=1, dist_sync_on_step=False):\n",
        "        super().__init__(dist_sync_on_step=dist_sync_on_step)\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        self.add_state(\"total\", default=torch.tensor(0, dtype=torch.float32), dist_reduce_fx=\"sum\")\n",
        "        self.add_state(\"nbr_sample\", default=torch.tensor(0), dist_reduce_fx=\"sum\")\n",
        "\n",
        "    def update(self, preds: torch.Tensor, target: torch.Tensor):\n",
        "        current_nbr_sample, current_nbr_category = preds.shape\n",
        "        if current_nbr_category != self.num_classes:\n",
        "          raise AttributeError(\"`num_classes` != `current_nbr_category` detected in `pred` parameter\")\n",
        "\n",
        "        current_loss_per_pred = torch.absolute(target - preds)\n",
        "        current_hamming_loss = current_loss_per_pred.sum()\n",
        "\n",
        "        self.total += current_hamming_loss.float()\n",
        "        self.nbr_sample += current_nbr_sample\n",
        "\n",
        "    def compute(self):\n",
        "        return self.total/(self.num_classes*self.nbr_sample)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-03T11:50:09.474463Z",
          "iopub.execute_input": "2024-06-03T11:50:09.474767Z",
          "iopub.status.idle": "2024-06-03T11:50:09.485304Z",
          "shell.execute_reply.started": "2024-06-03T11:50:09.474742Z",
          "shell.execute_reply": "2024-06-03T11:50:09.483884Z"
        },
        "trusted": true,
        "id": "C1SniQCIu2Oe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RebalancedHammingLossWithoutThreshold(Metric):\n",
        "    def __init__(self, num_classes=1, average=\"macro\", dist_sync_on_step=False):\n",
        "        super().__init__(dist_sync_on_step=dist_sync_on_step)\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        # average = \"macro\" or None\n",
        "        self.average = average\n",
        "\n",
        "        # Nombre de positif 1 & negatif 0 par categorie\n",
        "        self.add_state(\n",
        "            \"number_positive\",\n",
        "            default=torch.tensor([0 for _ in range(num_classes)]),\n",
        "            dist_reduce_fx=\"sum\",\n",
        "        )\n",
        "        self.add_state(\n",
        "            \"number_negative\",\n",
        "            default=torch.tensor([0 for _ in range(num_classes)]),\n",
        "            dist_reduce_fx=\"sum\",\n",
        "        )\n",
        "\n",
        "        self.add_state(\n",
        "            \"hamming_loss_positive\",\n",
        "            default=torch.tensor([0.0 for _ in range(num_classes)]),\n",
        "            dist_reduce_fx=\"sum\",\n",
        "        )\n",
        "        self.add_state(\n",
        "            \"hamming_loss_negative\",\n",
        "            default=torch.tensor([0.0 for _ in range(num_classes)]),\n",
        "            dist_reduce_fx=\"sum\",\n",
        "        )\n",
        "\n",
        "        self.add_state(\"nbr_sample\", default=torch.tensor(0), dist_reduce_fx=\"sum\")\n",
        "\n",
        "    def update(self, preds: torch.Tensor, target: torch.Tensor):\n",
        "        current_nbr_sample, current_nbr_category = preds.shape\n",
        "        if current_nbr_category != self.num_classes:\n",
        "            raise AttributeError(\n",
        "                \"`num_classes` != `current_nbr_category` detected in `pred` parameter\"\n",
        "            )\n",
        "\n",
        "        # Nombre de positif 1 & negatif 0 par categorie\n",
        "        current_number_positive = target.sum(axis=0)\n",
        "        current_number_negative = current_nbr_sample - target.sum(axis=0)\n",
        "\n",
        "        self.number_positive += current_number_positive.int()\n",
        "        self.number_negative += current_number_negative.int()\n",
        "\n",
        "        self.nbr_sample += current_nbr_sample\n",
        "\n",
        "        for class_id in range(self.num_classes):\n",
        "            positive_filter = target[:, class_id] == 1\n",
        "            negative_filter = target[:, class_id] == 0\n",
        "\n",
        "            target_vector = target[:, class_id]\n",
        "            preds_vector = preds[:, class_id]\n",
        "\n",
        "            # Filtered vector\n",
        "            ## Target\n",
        "            pos_filtered_target_vector = target_vector[positive_filter]\n",
        "            neg_filtered_target_vector = target_vector[negative_filter]\n",
        "            ## Preds\n",
        "            pos_filtered_preds_vector = preds_vector[positive_filter]\n",
        "            neg_filtered_preds_vector = preds_vector[negative_filter]\n",
        "\n",
        "            # Hamming Loss without Threshold\n",
        "            hamming_loss_on_positive = torch.absolute(\n",
        "                pos_filtered_target_vector - pos_filtered_preds_vector\n",
        "            )\n",
        "            hamming_loss_on_negative = torch.absolute(\n",
        "                neg_filtered_target_vector - neg_filtered_preds_vector\n",
        "            )\n",
        "\n",
        "            self.hamming_loss_positive[class_id] += hamming_loss_on_positive.sum()\n",
        "            self.hamming_loss_negative[class_id] += hamming_loss_on_negative.sum()\n",
        "\n",
        "    def compute(self):\n",
        "        factor_pos = self.nbr_sample / (2 * self.number_positive)\n",
        "        factor_neg = self.nbr_sample / (2 * self.number_negative)\n",
        "\n",
        "        rebalanced_hamming_loss_per_class = torch.multiply(\n",
        "            self.hamming_loss_positive, factor_pos\n",
        "        ) + torch.multiply(self.hamming_loss_negative, factor_neg)\n",
        "        if self.average == \"macro\":\n",
        "            return rebalanced_hamming_loss_per_class.sum() / (\n",
        "                self.nbr_sample * self.num_classes\n",
        "            )\n",
        "        return rebalanced_hamming_loss_per_class / (self.nbr_sample)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-03T11:50:10.129005Z",
          "iopub.execute_input": "2024-06-03T11:50:10.129605Z",
          "iopub.status.idle": "2024-06-03T11:50:10.150788Z",
          "shell.execute_reply.started": "2024-06-03T11:50:10.129567Z",
          "shell.execute_reply": "2024-06-03T11:50:10.14992Z"
        },
        "trusted": true,
        "id": "nPDNfzVZu2Oe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = len(LABEL_LIST)\n",
        "print(num_classes)\n",
        "train_metric_dict = dict()\n",
        "\n",
        "# AUROC Macro\n",
        "auroc_macro = AUROC(task=\"multilabel\",num_labels=num_classes, average=\"macro\")\n",
        "# auroc_macro = AUROC(\"MULTICLASS\",num_classes=num_classes, average=\"macro\")\n",
        "train_metric_dict[\"auroc_macro\"] = auroc_macro\n",
        "\n",
        "# # AUROC per class\n",
        "# auroc_per_class = AUROC(\"MULTICLASS\",num_classes=num_classes, average=None)\n",
        "auroc_per_class = AUROC(task=\"multilabel\",num_labels=num_classes, average=None)\n",
        "train_metric_dict[\"auroc_per_class\"] = auroc_per_class\n",
        "\n",
        "\n",
        "# # F1 score global\n",
        "# F1 score per class\n",
        "f1 = F1Score(task=\"multilabel\",num_labels=6, average=\"macro\")\n",
        "train_metric_dict[\"f1\"] = f1\n",
        "\n",
        "# F1 score per class\n",
        "f1_per_calss = F1Score(task=\"multilabel\",num_labels=6, average=None)\n",
        "train_metric_dict[\"f1_per_calss\"] = f1_per_calss\n",
        "\n",
        "# Hamming Distance without Threshold\n",
        "hamming_loss_woutt = HammingLossWithoutThreshold(num_classes=num_classes)\n",
        "train_metric_dict[\"hamming_loss_without_threshold\"] = hamming_loss_woutt\n",
        "\n",
        "# Rebalanced Hamming Distance without Threshold macro\n",
        "rebalanced_hamming_loss_woutt_macro = RebalancedHammingLossWithoutThreshold(\n",
        "    num_classes=num_classes, average=\"macro\"\n",
        ")\n",
        "train_metric_dict[\n",
        "    \"rebalanced_hamming_loss_without_threshold_macro\"\n",
        "] = rebalanced_hamming_loss_woutt_macro\n",
        "\n",
        "# Rebalanced Hamming Distance without Threshold macro\n",
        "rebalanced_hamming_loss_woutt_per_class = RebalancedHammingLossWithoutThreshold(\n",
        "    num_classes=num_classes, average=None\n",
        ")\n",
        "train_metric_dict[\n",
        "    \"rebalanced_hamming_loss_without_threshold_per_class\"\n",
        "] = rebalanced_hamming_loss_woutt_per_class"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-03T11:50:10.719363Z",
          "iopub.execute_input": "2024-06-03T11:50:10.719732Z",
          "iopub.status.idle": "2024-06-03T11:50:10.734584Z",
          "shell.execute_reply.started": "2024-06-03T11:50:10.719702Z",
          "shell.execute_reply": "2024-06-03T11:50:10.733673Z"
        },
        "trusted": true,
        "id": "bTnIwjnpu2Oe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_metric = MetricCollection(train_metric_dict)\n",
        "train_metric.to(device)\n",
        "\n",
        "validation_metric = train_metric.clone()\n",
        "validation_metric.to(device)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-03T11:50:11.396202Z",
          "iopub.execute_input": "2024-06-03T11:50:11.397181Z",
          "iopub.status.idle": "2024-06-03T11:50:11.435348Z",
          "shell.execute_reply.started": "2024-06-03T11:50:11.39714Z",
          "shell.execute_reply": "2024-06-03T11:50:11.434372Z"
        },
        "trusted": true,
        "id": "mzpXiTSCu2Oe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def serialize(object_to_serialize: Any, ensure_ascii: bool = True) -> str:\n",
        "    \"\"\"\n",
        "    Serialize any object, i.e. convert an object to JSON\n",
        "    Args:\n",
        "        object_to_serialize (Any): The object to serialize\n",
        "        ensure_ascii (bool, optional): If ensure_ascii is true (the default), the output is guaranteed to have all incoming non-ASCII characters escaped. If ensure_ascii is false, these characters will be output as-is. Defaults to True.\n",
        "    Returns:\n",
        "            str: string of serialized object (JSON)\n",
        "    \"\"\"\n",
        "\n",
        "    def dumper(obj: Any) -> Union[str, Dict]:\n",
        "        \"\"\"\n",
        "        Function called recursively by json.dumps to know how to serialize an object.\n",
        "        For example, for datetime, we try to convert it to ISO format rather than\n",
        "        retrieve the list of attributes defined in its object.\n",
        "        Args:\n",
        "            obj (Any): The object to serialize\n",
        "        Returns:\n",
        "            Union[str, Dict]: Serialized object\n",
        "        \"\"\"\n",
        "        if isinstance(obj, torch.Tensor):\n",
        "            return obj.cpu().numpy().tolist()\n",
        "        elif hasattr(obj, \"__dict__\"):\n",
        "            return obj.__dict__\n",
        "        return str(obj)\n",
        "\n",
        "    return json.dumps(object_to_serialize, default=dumper, ensure_ascii=ensure_ascii)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-03T11:50:12.278789Z",
          "iopub.execute_input": "2024-06-03T11:50:12.279475Z",
          "iopub.status.idle": "2024-06-03T11:50:12.286539Z",
          "shell.execute_reply.started": "2024-06-03T11:50:12.279438Z",
          "shell.execute_reply": "2024-06-03T11:50:12.285568Z"
        },
        "trusted": true,
        "id": "2hZP4v5du2Oe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def export_metric(metric_collection, **kwargs):\n",
        "    \"\"\"\n",
        "    Export MetricCollection to json file\n",
        "\n",
        "    Args:\n",
        "        metric_collection: MetricCollection\n",
        "        **kwargs: field to add in json line\n",
        "    \"\"\"\n",
        "    with open(METRIC_FILE_PATH, \"a\") as f:\n",
        "        metric_collection_value = metric_collection.compute()\n",
        "        metric_collection_value.update(kwargs)\n",
        "        serialized_value = serialize(metric_collection_value)\n",
        "        f.write(serialized_value)\n",
        "        f.write(\"\\n\")\n",
        "    logger.success(\"Metrics are exported !\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-03T11:50:12.90254Z",
          "iopub.execute_input": "2024-06-03T11:50:12.903387Z",
          "iopub.status.idle": "2024-06-03T11:50:12.908865Z",
          "shell.execute_reply.started": "2024-06-03T11:50:12.903335Z",
          "shell.execute_reply": "2024-06-03T11:50:12.90787Z"
        },
        "trusted": true,
        "id": "vNYYXXAiu2Oe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch(epoch_id=None):\n",
        "    model.train()\n",
        "    logger.info(f\"START EPOCH {epoch_id=}\")\n",
        "\n",
        "    progress = tqdm(train_dataloader, desc='training batch...', leave=False)\n",
        "    for batch_id, batch in enumerate(progress):\n",
        "        if batch_id % 1_000 == 0:\n",
        "            valid_epoch(epoch_id=epoch, batch_id=batch_id)\n",
        "\n",
        "        logger.trace(f\"{batch_id=}\")\n",
        "        token_list_batch = batch[\"ids\"].to(device)\n",
        "        attention_mask_batch = batch[\"mask\"].to(device)\n",
        "        label_batch = batch[\"labels\"].to(device)\n",
        "\n",
        "        # Reset gradient\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Predict\n",
        "        prediction_batch = model(token_list_batch, attention_mask_batch)\n",
        "        transformed_prediction_batch = prediction_batch.squeeze()\n",
        "\n",
        "        # Loss\n",
        "        loss = criterion(transformed_prediction_batch.to(torch.float32), label_batch.to(torch.float32))\n",
        "\n",
        "        # Metrics\n",
        "        proba_prediction_batch = torch.sigmoid(transformed_prediction_batch)\n",
        "        train_metrics_collection_dict = train_metric(proba_prediction_batch.to(torch.float32), label_batch.to(torch.int32))\n",
        "        logger.trace(train_metrics_collection_dict)\n",
        "\n",
        "        # Backprop\n",
        "        loss.backward()\n",
        "        # gradient clip\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update progress bar description\n",
        "        progress_description = \"Train Loss : {loss:.4f} - Train AUROC : {acc:.4f}\"\n",
        "        auc_roc = float(train_metrics_collection_dict[\"auroc_macro\"])\n",
        "        progress_description = progress_description.format(loss=loss.item(), acc=auc_roc)\n",
        "        progress.set_description(progress_description)\n",
        "\n",
        "    logger.info(f\"END EPOCH {epoch_id=}\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-03T11:50:13.674139Z",
          "iopub.execute_input": "2024-06-03T11:50:13.674875Z",
          "iopub.status.idle": "2024-06-03T11:50:13.686204Z",
          "shell.execute_reply.started": "2024-06-03T11:50:13.674841Z",
          "shell.execute_reply": "2024-06-03T11:50:13.684939Z"
        },
        "trusted": true,
        "id": "Fyl-0kmdu2Oe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def valid_epoch(epoch_id=None, batch_id=None):\n",
        "    model.eval()\n",
        "    logger.info(f\"START VALIDATION {epoch_id=}{batch_id=}\")\n",
        "    validation_metric.reset()\n",
        "\n",
        "    loss_list = []\n",
        "    prediction_list = torch.Tensor([])\n",
        "    target_list = torch.Tensor([])\n",
        "\n",
        "\n",
        "    progress = tqdm(validation_dataloader, desc=\"valid batch...\", leave=False)\n",
        "    for _, batch in enumerate(progress):\n",
        "\n",
        "        token_list_batch = batch[\"ids\"].to(device)\n",
        "        attention_mask_batch = batch[\"mask\"].to(device)\n",
        "        label_batch = batch[\"labels\"].to(device)\n",
        "\n",
        "        # Predict\n",
        "        prediction_batch = model(token_list_batch, attention_mask_batch)\n",
        "\n",
        "        transformed_prediction_batch = prediction_batch.squeeze()\n",
        "\n",
        "        # Loss\n",
        "        loss = criterion(\n",
        "            transformed_prediction_batch.to(torch.float32),\n",
        "            label_batch.to(torch.float32),\n",
        "        )\n",
        "\n",
        "        loss_list.append(loss.item())\n",
        "\n",
        "        proba_prediction_batch = torch.sigmoid(transformed_prediction_batch)\n",
        "        prediction_list = torch.concat(\n",
        "            [prediction_list, proba_prediction_batch.cpu()]\n",
        "        )\n",
        "        target_list = torch.concat([target_list, label_batch.cpu()])\n",
        "\n",
        "        # Metrics\n",
        "        validation_metric(proba_prediction_batch.to(torch.float32), label_batch.to(torch.int32))\n",
        "\n",
        "    loss_mean = np.mean(loss_list)\n",
        "    logger.trace(validation_metric.compute())\n",
        "    logger.info(f\"END VALIDATION {epoch_id=}{batch_id=}\")\n",
        "    export_metric(validation_metric, epoch_id=epoch_id, batch_id=batch_id, loss=loss_mean)@torch.no_grad()\n",
        "def valid_epoch(epoch_id=None, batch_id=None):\n",
        "    model.eval()\n",
        "    logger.info(f\"START VALIDATION {epoch_id=}{batch_id=}\")\n",
        "    validation_metric.reset()\n",
        "\n",
        "    loss_list = []\n",
        "    prediction_list = torch.Tensor([])\n",
        "    target_list = torch.Tensor([])\n",
        "\n",
        "\n",
        "    progress = tqdm(validation_dataloader, desc=\"valid batch...\", leave=False)\n",
        "    for _, batch in enumerate(progress):\n",
        "\n",
        "        token_list_batch = batch[\"ids\"].to(device)\n",
        "        attention_mask_batch = batch[\"mask\"].to(device)\n",
        "        label_batch = batch[\"labels\"].to(device)\n",
        "\n",
        "        # Predict\n",
        "        prediction_batch = model(token_list_batch, attention_mask_batch)\n",
        "\n",
        "        transformed_prediction_batch = prediction_batch.squeeze()\n",
        "\n",
        "        # Loss\n",
        "        loss = criterion(\n",
        "            transformed_prediction_batch.to(torch.float32),\n",
        "            label_batch.to(torch.float32),\n",
        "        )\n",
        "\n",
        "        loss_list.append(loss.item())\n",
        "\n",
        "        proba_prediction_batch = torch.sigmoid(transformed_prediction_batch)\n",
        "        prediction_list = torch.concat(\n",
        "            [prediction_list, proba_prediction_batch.cpu()]\n",
        "        )\n",
        "        target_list = torch.concat([target_list, label_batch.cpu()])\n",
        "\n",
        "        # Metrics\n",
        "        validation_metric(proba_prediction_batch.to(torch.float32), label_batch.to(torch.int32))\n",
        "\n",
        "    loss_mean = np.mean(loss_list)\n",
        "    logger.trace(validation_metric.compute())\n",
        "    logger.info(f\"END VALIDATION {epoch_id=}{batch_id=}\")\n",
        "    export_metric(validation_metric, epoch_id=epoch_id, batch_id=batch_id, loss=loss_mean)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-03T11:50:14.23375Z",
          "iopub.execute_input": "2024-06-03T11:50:14.234435Z",
          "iopub.status.idle": "2024-06-03T11:50:14.249961Z",
          "shell.execute_reply.started": "2024-06-03T11:50:14.234401Z",
          "shell.execute_reply": "2024-06-03T11:50:14.249144Z"
        },
        "trusted": true,
        "id": "DIDrYnFBu2Oe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()\n",
        "progress =  tqdm(range(1,NUM_EPOCHS+1), desc='training epoch...', leave=True)\n",
        "for epoch in progress:\n",
        "    # Train\n",
        "    train_epoch(epoch_id=epoch)\n",
        "\n",
        "    # Validation\n",
        "    valid_epoch(epoch_id=epoch)\n",
        "\n",
        "    # Save\n",
        "    torch.save(model, MODEL_FILE_PATH)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-03T10:04:34.070968Z",
          "iopub.status.idle": "2024-06-03T10:04:34.071412Z",
          "shell.execute_reply.started": "2024-06-03T10:04:34.07118Z",
          "shell.execute_reply": "2024-06-03T10:04:34.071197Z"
        },
        "trusted": true,
        "id": "28e0Pn0cu2Oe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation"
      ],
      "metadata": {
        "id": "-U4eqjr-u2Of"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# try:\n",
        "#     del train_df\n",
        "#     del validation_df\n",
        "# except NameError:\n",
        "#     logger.warning(\"Train DataFrame & Validation DataFrame already deleted\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-03T10:04:34.072493Z",
          "iopub.status.idle": "2024-06-03T10:04:34.072983Z",
          "shell.execute_reply.started": "2024-06-03T10:04:34.072718Z",
          "shell.execute_reply": "2024-06-03T10:04:34.072736Z"
        },
        "trusted": true,
        "id": "abW-pqzJu2Of"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "w3Ckgtz0u2Of"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}